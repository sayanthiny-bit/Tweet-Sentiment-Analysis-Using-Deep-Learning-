# Tweet-Sentiment-Analysis-Using-Deep-Learning-
# ğŸ§  Tweet Classification with 2-Layer LSTM (Trained Embeddings)

This project implements a deep learning model using a **2-layer LSTM architecture** to classify tweets as personal health mentions or non-mentions. The model uses a **trainable embedding layer** (no pre-trained word vectors).

---

## ğŸ“Œ Objective

To build and train a binary classification model that:
- Classifies tweets into personal health mentions (label 1) or not (label 0)
- Uses a trainable embedding layer initialized by Keras

---

## ğŸ› ï¸ Technologies Used

- Python
- Keras / TensorFlow
- NumPy
- Pandas for preprocessing

---

## ğŸ§¾ Dataset

- Input: Cleaned tweets
- Labels:
  - `1` â†’ Personal health mention  
  - `0` â†’ Not a personal health mention

---

## ğŸ“ˆ Model Architecture

```python
Embedding (trainable)
â†’ LSTM Layer (64 units, return_sequences=True)
â†’ Dropout (0.3)
â†’ LSTM Layer (32 units)
â†’ Dropout (0.3)
â†’ Dense Layer (1 unit, sigmoid activation)

Embedding Details

Keras Embedding() layer

Embedding dimension: 100

Input size: vocab_size (from Tokenizer)

Output: Dense 64D vector for each token

Trainable: Yes (learns during training)

Training Configuration
Loss: Binary Crossentropy

Optimizer: Adam

Epochs: 10

Batch Size: 128

Validation: Held-out test set

ğŸ“Š Evaluation Results
Accuracy Achieved: ~77% - 81% (based on tuning)

Correct vs. Wrong predictions printed after evaluation

âœ… Possible Improvements
Add pre-trained embeddings (e.g., GloVe, Word2Vec) (process)

Use Bi-LSTM or GRU for performance comparison

Tune embedding size, LSTM units, dropout rate






